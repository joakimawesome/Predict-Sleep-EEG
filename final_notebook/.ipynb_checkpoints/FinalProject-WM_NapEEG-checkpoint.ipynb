{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UC San Diego: Neural Data Science\n",
    "## Classifying Sleep Based on Prior Working Memory Task Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions\n",
    "\n",
    "Place an `X` in the appropriate bracket below to specify if you would like your group's project to be made available to the public. (Note that student names will be included (but PIDs will be scraped from any groups who include their PIDs).\n",
    "\n",
    "* [  ] YES - make available\n",
    "* [ X ] NO - keep private"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Anjali Srinivasan\n",
    "- Aaditya Prasad\n",
    "- Joakim Nguyen\n",
    "- Yohan Kim\n",
    "- Kenton Guarian\n",
    "- Mary Kovic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we investigated whether it is possible to determine if an individual performed a high-load or low-load working memory task prior to sleep based on EEG data from their sleep. This analysis made use of the LISC database to determine what features of the EEG data would be the most applicable to working memory. After mining for relevant terms, we cleaned the data, extracted our features of interest, and performed k-means clustering. We then evaluated whether the clustering successfully separated the data by what working memory task the participant performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='research_question'></a>\n",
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we predict whether a subject performed a high-load or low-load working memory task prior to sleeping based on EEG signals from that nap?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='background'></a>\n",
    "\n",
    "## Background & Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   After investigating the Nap EEG data, we became curious about whether we could use the scalp EEG signals to determine whether the participant performed a high-load or a low-load working memory task prior to sleeping. We knew from prior knowledge that sleep has a role in consolidating information and memories. Additionally, Ellenbogen et al (2006) demonstrated that sleep protects declarative memories and has a role in memory recall [1].\n",
    "    \n",
    "   Sleep deprivation can have an effect on cognitive function and performance - in Choo et al (2005), participants that were sleep deprived performed worse on the n-back working memory task than their rested counterparts [2]. Additionally, Gradisar et al (2016) observed that adolescents that are sleep deprived (less than 8 hours of sleep) or have a sufficient amount of sleep (greater than 9 hours) perform worse than adolescents with 8-9 hours of sleep on various working memory tasks [3]. The effect sleep has on working memory performance is clearly complex and nuanced.\n",
    "\n",
    "   Looking at this data, it is clear that sleep can have a substantial effect on working memory task performance, and that the brain has a role in consolidating memories during sleep. Thus, we are curious about whether there is a significant difference in brain activity after performing a working memory task. Pugin et al (2015) found that after undergoing working memory training, participants had an increase of slow wave activity during sleep [4]. We are curious as to whether a difference exists in nap EEG recordings without training and are investigating whether the complexity of the working memory task has a noticeable effect on the nap EEG recordings.\n",
    "\n",
    "References:\n",
    "- 1) https://www.sciencedirect.com/science/article/pii/S0960982206016071\n",
    "- 2) https://www.sciencedirect.com/science/article/pii/S1053811904007074?via%3Dihub\n",
    "- 3) https://link.springer.com/article/10.1111/j.1479-8425.2008.00353.x\n",
    "- 4) https://academic.oup.com/sleep/article/38/4/607/2416936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our hypothesis is that there will be more activity in the brain during sleep after a high-load working memory task compared to a low-load working memory task. Thus, we predict that it is possible to accurately cluster the data by working memory task load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LISC\n",
    "- Link to the dataset: https://lisc-tools.github.io/lisc/auto_tutorials/index.html\n",
    "- Number of observations: all scientific literature\n",
    "LISC is a tool that allows users to perform text mining on scientific articles and various analyses. Users can query databases for terms of interest and learn how many articles contain those terms.\n",
    "\n",
    "Nap EEG\n",
    "- Link to the dataset: https://osf.io/chav7/\n",
    "- Number of observations: 64-channel continuously scalped EEG data for 19 participants over 36 datasets\n",
    "\n",
    "To combine these datasets, we will be utilizing LISC to find important terms that co-occur between sleep and working memory. We will then attempt to use these terms to determine which features to extract from the data when performing clustering analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's import the necessary packages\n",
    "!pip install numpy mne pandas matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making necessary directories\n",
    "!mkdir original_eeg\n",
    "!mkdir cleaned_eeg\n",
    "!mkdir spindle_files\n",
    "!mkdir lisc_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import copy\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from lisc import Counts\n",
    "from lisc.utils.db import SCDB\n",
    "from lisc.utils.io import save_object\n",
    "from lisc.plts.counts import plot_matrix\n",
    "# from wordcloud import WordCloud, get_single_color_func\n",
    "\n",
    "# Local data imports\n",
    "from data.lisc_db.terms.terminology import waves, eeg_features_2, additional_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to retreive the EEG data. To do this, we will need to use the download links provided in the file 'available_subjects.csv' (provided in the current working directory) to retrieve the eeg data.\n",
    "\n",
    "Note for available_subjects.csv:\n",
    "This file was originally retreived from the github page for the original paper.\n",
    "Link to github: https://github.com/nmningmei/Get_Sleep_data\n",
    "However, there were multiple cells in this file that had typos and naming errors - we edited these errors manually. The 'available_subjects.csv' file provided with this project is the fixed version of the one from the original github.\n",
    "\n",
    "The eeg data will be saved in the 'original_eeg' directory.\n",
    "\n",
    "Samples with 'l2' in the name are recordings taken after a low-load working memory task. Samples with 'l5' in the name are recordings taken after a high-load working memory task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_df = pd.read_csv('available_subjects.csv')\n",
    "\n",
    "for link in subjects_df['link']:\n",
    "    link = link.strip()\n",
    "    !wget -P original_eeg/ {link}\n",
    "\n",
    "# store all vhdr filenames to iterate through for cleaning\n",
    "vhdrs = [name for name in subjects_df['name'] if name.endswith('.vhdr')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to retreive the spindle annotation data. We will be using this data in exploratory analyses and will attempt to use these labels to classify the data.\n",
    "\n",
    "The spindle annotation data will be saved in the 'spindle_files' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name,link in zip(subjects_df['annotation_file_name'],subjects_df['annotation_file_link']):\n",
    "    save_file_name = \".spindle_files/\" + file_name\n",
    "    !wget -O {save_file_name} {link}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to clean the eeg data.\n",
    "\n",
    "While exploring the dataset, we noticed a discrepancy between the code provided in the github (link provided in the 'Data Wrangling' section) and the code provided in the original dataset (link provided in the dataset information section). The code provided in the original dataset filtered the data using a bandpass filter, ran it through a notch filter, and performed ICA (independent component analysis) on it.\n",
    "\n",
    "In contrast, the code on the github did not run the samples through ICA. We weren't sure which to follow - if ICA was not necessary we did not want to run it, as it is a computationally intensive algorithm that takes some time to run,\n",
    "\n",
    "To clarify whether we should run each sample through ica, we contacted Ning Mei, one of the authors on the paper published with the dataset. Mei informed us that ICA has little to no effect on the data from this dataset, so it is not worth the time to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_import(signal_str):\n",
    "    \n",
    "    # read in raw eeg file and set channel types\n",
    "    raw = mne.io.read_raw_brainvision(signal_str)\n",
    "    \n",
    "    # Set channel type as said in original research paper\n",
    "    channel_types = {'LOc':'eog','ROc':'eog','Aux1':'misc'}\n",
    "    raw.set_channel_types(channel_types) # Set channel type as said in original research paper\n",
    "    \n",
    "    \n",
    "    raw_ref ,_  = mne.set_eeg_reference(raw, ref_channels='average', projection=True,)\n",
    "    raw_ref.apply_proj() # it might tell you it already has been re-referenced, but do it anyway\n",
    "\n",
    "    # read standard montage - montage is important for visualization\n",
    "    montage = mne.channels.make_standard_montage('standard_1020');#montage.plot()\n",
    "    raw.set_montage(montage)\n",
    "    \n",
    "    # print some information about the data\n",
    "    # print(raw.info)\n",
    "    # The graph's voltage is microvolt, but actually it should be in volts, so we convert that\n",
    "    scalingDict=dict(mag=1e-12, grad=4e-11, eeg=20e-6, eog=150e-6, ecg=5e-4, emg=1e-3, ref_meg=1e-12,\n",
    "                     misc=1e-3, stim=1, resp=1, chpi=1e-4)\n",
    "    \n",
    "    # plot and reload the data\n",
    "    raw.plot(start=0., duration = 10.0, n_channels=20, scalings = scalingDict)\n",
    "    raw.load_data()\n",
    "    \n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_channels(raw):\n",
    "    \n",
    "    # create list of channels to retain in raw\n",
    "    chan_list=raw.ch_names[:-2]# exclude 'AUX','STIM' channels\n",
    "    if 'LOc' not in chan_list:\n",
    "        chan_list.append('LOc')\n",
    "    if 'ROc' not in chan_list:\n",
    "        chan_list.append('ROc')\n",
    "    \n",
    "    # pick useful channels to use; which means to remove unnecessary channels like AUX\n",
    "    raw.pick_channels(chan_list)\n",
    "    raw.set_channel_types({'LOc':'eog','ROc':'eog'})\n",
    "    raw.pick_channels(chan_list)\n",
    "    \n",
    "    # pick channels by type (eeg)\n",
    "    picks = mne.pick_types(raw.info,meg=False,eeg=True,eog=False,stim=False) # Only use eeg\n",
    "    \n",
    "    # band-pass filter the data so that all signal fits between low frequency of 200 and\n",
    "    # high frequency of 1\n",
    "    raw.filter(l_freq=200, h_freq=1,l_trans_bandwidth=0.01, h_trans_bandwidth='auto', filter_length='auto',\n",
    "               picks=picks)\n",
    "    noise_cov = mne.compute_raw_covariance(raw.set_eeg_reference(), picks=picks)\n",
    "    \n",
    "    # apply notch filter(from 60 to 240, with interval of 60)\n",
    "    raw.notch_filter(np.arange(60,241,60), picks=picks)\n",
    "    return picks, noise_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cleaned_result(clean_raw, signal_str):\n",
    "\n",
    "    # save cleaned eeg files as .fif into the cleaned_eeg directory\n",
    "    save_dir = \"./cleaned_eeg/\"\n",
    "    \n",
    "    savename=signal_str[:len(signal_str)-5:] + '.fif'\n",
    "    clean_raw.save(save_dir + savename, # file name\n",
    "               buffer_size_sec=None, # size of data chunks in seconds. \n",
    "               proj=False, # always false\n",
    "               overwrite = False, # Default to \"false\" so you don't mess up\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each eeg sample\n",
    "for vhdr in vhdrs:\n",
    "    signal_str = \"./original_eeg/\" + vhdr # Path to Signal\n",
    "    \n",
    "    # import raw data\n",
    "    raw = raw_import(signal_str)\n",
    "    \n",
    "    # filter data\n",
    "    picks, noise_cov = filter_channels(raw)\n",
    "    \n",
    "    # save cleaned data as a .fif file\n",
    "    save_cleaned_result(raw, vhdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is cleaned, let's take a look at it from before and after filtering. We'll use the file suj11_l2nap_day2 as our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, plot the prefiltered data and display basic information\n",
    "raw = raw_import(\"original_eeg/suj11_l2nap_day2.vhdr\")\n",
    "raw.plot()\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's look at the filtered data\n",
    "clean_raw = mne.io.read_raw_fif(\"cleaned_eeg/suj11_l2nap_day2.fif\")\n",
    "clean_raw.plot()\n",
    "clean_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the filtering has removed much of the noise present in our data. Great! Now we're able to perform some analysis and extract relevant features from the data.\n",
    "\n",
    "However, first, we must ask the question \"What features do we extract?\" To answer this, let's take a small break from the Nap EEG data and turn our attention towards LISC. When starting this project, we were unsure of what terms were related between sleep and working memory. Using LISC, we were able to narrow down possible terms to look at in our classification analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_save(counts, filename:str, db):\n",
    "    '''Executes run_collection() on Counts object and saves it to SCDB (database).'''\n",
    "    # Run co-occurrences of search terms.\n",
    "    counts.run_collection()\n",
    "\n",
    "    # Save the data to SCDB.\n",
    "    main_dir = os.path.dirname(os.getcwd())\n",
    "    db_dir = os.path.join(main_dir, 'lisc_analysis/', db.get_folder_path('counts'))\n",
    "    save_object(counts, filename, directory=db_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO change this to save files to lisc_analysis directory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_counts(filename:str, db):\n",
    "    '''Returns Counts object from SCDB (database).'''\n",
    "    main_dir = os.path.dirname(os.getcwd())\n",
    "    data_rpath = db.get_file_path('counts', filename)\n",
    "    data_abspath = os.path.join(main_dir, 'lisc_analysis/', data_rpath)\n",
    "    return pd.read_pickle(data_abspath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top(counts, top_n:int=None, thresh:float=None) -> list[tuple]:\n",
    "    '''Returns the top 'n' co-occurrence or top percentile co-occurrences terms'''\n",
    "    counts = copy.deepcopy(counts)\n",
    "    len_A = len(counts.terms['A'].terms)\n",
    "    len_B = len(counts.terms['B'].terms)\n",
    "\n",
    "    top = []\n",
    "    if top_n:\n",
    "        thresh = 0\n",
    "\n",
    "    assert 0 < thresh < 1, \"Argument 'thresh' must be float between 0 and 1.\"\n",
    "\n",
    "    # Weed out scores that are below certain threshold\n",
    "    for i in range(len_A):\n",
    "        for j in range(len_B):\n",
    "            if counts.score[i][j] > counts.score.max() * thresh:\n",
    "                top.append([counts.terms['A'].terms[i][0], counts.terms['B'].terms[j][0], counts.score[i][j]])\n",
    "\n",
    "    top = sorted(top, key=lambda x: x[2], reverse=True)\n",
    "    if top_n:\n",
    "        top = top[:top_n]\n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lisc SCDB (database) object.\n",
    "db = SCDB('lisc_db')\n",
    "db.gen_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look more closely at terms associated with working memory in literature. To do this, we will be making use of terms from the Cognitive Atlas - https://www.cognitiveatlas.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch html content from Cognitive Atlas website\n",
    "url = \"https://www.cognitiveatlas.org/concepts/categories/all\"\n",
    "page = requests.get(url)\n",
    "data = page.text\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "# Let's cut down html into the categories of concepts\n",
    "page_content = soup.find('div', id='pagecontent')\n",
    "categories = page_content.find_all('h3')\n",
    "categories = list(enumerate([category.get_text() for category in categories]))\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which category by index.\n",
    "category_idx = 5 # is Learning and Memory\n",
    "\n",
    "# Obtain all the concept terms in the category you chose.\n",
    "category = soup.find(string=categories[category_idx][1]).parent\n",
    "concepts = category.find_next_sibling('div').find_all('a')\n",
    "concepts = [term.get_text() for term in concepts]\n",
    "\n",
    "# Remove some words that aren't productive in \"concepts\".\n",
    "remove = ['memory', 'skill', 'learning', 'knowledge']\n",
    "concepts = [concept for concept in concepts if concept not in remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run co-occurances of \"working memory\" to the concepts.\n",
    "terms_a = [\"working memory\"]\n",
    "terms_b = sorted(concepts) # alphabetical order\n",
    "\n",
    "counts = Counts()\n",
    "counts.add_terms(terms_a, dim='A')\n",
    "counts.add_terms(terms_b, dim='B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Counts data\n",
    "# run_and_save(counts, 'wm-memlrn.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the counts data and compute its scores.\n",
    "pickle1 = load_counts('wm-memlrn.p')\n",
    "pickle1.compute_score('normalize', dim='A')\n",
    "\n",
    "# Prepare a wordcloud   \n",
    "# LISC's wordcloud function won't suffice so we'll do it ourselves.\n",
    "###################################################################\n",
    "\n",
    "# Compile the words and frequencies into a dictionary to pass into the WordCloud object.\n",
    "# Conveniently, our data is already 1D.\n",
    "Dict = {}\n",
    "for word, score in zip(pickle1.terms['B'].terms, pickle1.score[0]):\n",
    "    Dict[word[0]] = score\n",
    "# Tweak the parameters to produce a beautiful wordcloud.\n",
    "wc = WordCloud(width=2560, height=1440, background_color='white').fit_words(Dict)\n",
    "color_func1 = get_single_color_func('deepskyblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(pickle1, attribute='score', cmap='blue', figsize=(30, 5), square=True) # kwargs sent to seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "plt.imshow(wc.recolor(color_func=color_func1), interpolation='bilinear')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what terms appear most frequently in working memory literature, let's investigate what terms co-occur between 'learning and memory' and 'EEG features'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Counts object\n",
    "terms_c = sorted(concepts)\n",
    "terms_d = sorted([wave + ' ' + feature for wave in waves for feature in eeg_features_2] + additional_features)\n",
    "terms_d.remove('slow wave')\n",
    "\n",
    "counts2 = Counts()\n",
    "counts2.add_terms(terms_c, dim='A')\n",
    "counts2.add_terms(terms_d, dim='B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_and_save(counts2, 'memlrn-eeg.p', db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the counts data and compute its scores\n",
    "pickle2 = load_counts('memlrn-eeg.p', db)\n",
    "pickle2.compute_score('normalize', dim='A')\n",
    "plot_matrix(pickle2, attribute='score', transpose=True, figsize=(30, 15), square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix is really big and generally unreadable. Let's narrow down our search a bit and find co-occurences between 'working memory' exclusively and 'EEG features'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare another wordclouod with \"working memory\" terms.\n",
    "# The top 12 words are selected.\n",
    "Dict = {}\n",
    "for word, score in zip(pickle1.terms['B'].terms, pickle1.score[0]):\n",
    "    Dict[word[0]] = score\n",
    "wc = WordCloud(width=2560, height=1440, max_words=12, background_color='white').fit_words(Dict)\n",
    "color_func1 = get_single_color_func('deepskyblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "plt.imshow(wc.recolor(color_func=color_func1), interpolation='bilinear')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Counts object\n",
    "# Pick top 12 terms from \"learning and memory\" to represent \"working memory\".\n",
    "# Pickle1 is from the first Counts analysis at the top of the LISC portion of this notebook.\n",
    "terms_e = list(zip(*pick_top(pickle1, top_n=12)))[1]\n",
    "terms_f = sorted([wave + ' ' + feature for wave in waves for feature in eeg_features_2] + additional_features)\n",
    "terms_f.remove('slow wave')\n",
    "\n",
    "counts3 = Counts()\n",
    "counts3.add_terms(terms_e, dim='A')\n",
    "counts3.add_terms(terms_f, dim='B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_and_save(counts3, \"wm-eeg.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the counts data and compute its scores\n",
    "pickle3 = load_counts('wm-eeg.p')\n",
    "pickle3.compute_score('normalize', dim='A')\n",
    "plot_matrix(pickle3, attribute='score', cmap='purple', figsize=(30, 8), square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, many of these working memory terms have co-occurences with different types of oscillations and waves. In sleep, each of these types of waves corresponds to a frequency range of scalp recordings. Seeing as how working memory seems to have a lot to do with different wavelengths and frequencies of brain activity, it is clear that extracting frequency data from each sample is a promising method to classifying our data.\n",
    "\n",
    "To do this, we'll need to determine the power spectrum of the data. This will give us distribution of the signal power over frequencies, which will demonstrate which frequencies were most represented in the EEG data.\n",
    "\n",
    "Let's look at the power spectrum for the data we visualized earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_raw.plot_psd(fmax=59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " TODO [insert info about plot here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what we're looking for, let's extract the power spectrum from each datapoint. We'll be using 'available_subjects.csv' to iterate through the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(eeg_file, crop=True):\n",
    "    \n",
    "    # read in cleaned eeg file and crop it to a maximum of 30 minutes\n",
    "    raw = mne.io.read_raw_fif(eeg_file, preload=True)\n",
    "    \n",
    "    # if crop is true, crop data to 1800 seconds\n",
    "    if crop:\n",
    "        raw.crop(tmax=1800)\n",
    "    \n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_data(data, spindle_file):\n",
    "    \n",
    "    events = pd.read_csv(spindle_file)\n",
    "    annotation_events = events[events['Annotation'] == 'spindle']\n",
    "    event_array = np.vstack([annotation_events['Onset'].values, [0] * annotation_events.shape[0],\n",
    "                             [1] * annotation_events.shape[0]]).T.astype(int)\n",
    "    \n",
    "    epochs = mne.Epochs(raw, events = event_array, event_id = {'spindle':1}, tmin = -0.5, \n",
    "                        tmax = 1.5, baseline = (-0.5,-0.2), preload = True, picks = picks, detrend = 1)\n",
    "    \n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_power_series(data):\n",
    "    \n",
    "    # run psd_welch on each dataset\n",
    "    # note: dimensions of returned matrix change depending on whether psd_welch is run on epoched data\n",
    "    pxx, fx = mne.time_frequency.psd_welch(data, fmin= 1., fmax = 50, n_fft=1000, n_overlap = 500)\n",
    "    \n",
    "    return pxx, fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eegs = np.array(subjects_df[subjects_df['name'].str.endswith('.eeg')]['name'])\n",
    "eeg_file_names = [eegs[i].split('.')[0] + '.fif' for i in range(len(eegs))]\n",
    "spindle_annotation_file_names = np.unique(np.array(subjects_df['annotation_file_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_file_dir = \"./cleaned_eeg/\"\n",
    "annotation_file_dir = \"./spindle_files/\"\n",
    "skip = [\"suj27_l5nap_day1.fif\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_power_series = []\n",
    "flat_power_series = []\n",
    "fxs = []\n",
    "labels = []\n",
    "\n",
    "for eeg_file_name, spindle_annotation_file_name in zip(eeg_file_names, spindle_annotation_file_names):\n",
    "    \n",
    "    # skip files in 'skip' directory - eeg signal is abnormal and should not be counted\n",
    "    if eeg_file_name in skip:\n",
    "        continue\n",
    "        \n",
    "    if 'l2' in eeg_file_name:\n",
    "        # annotate with '0' if low-load\n",
    "        labels.append(0)\n",
    "    else:\n",
    "        # annotate with '1' if low-load\n",
    "        labels.append(1)\n",
    "    \n",
    "    full_eeg_path = eeg_file_dir + eeg_file_name\n",
    "    full_spindle_annotation_path = annotation_file_dir + spindle_annotation_file_name\n",
    "    \n",
    "    # extract features from data epoched by spindle and epoch data by spindles\n",
    "    raw = import_data(full_eeg_path), crop=False)\n",
    "    epochs = epoch_data(raw, full_spindle_annotation_path)\n",
    "    \n",
    "    # extract power series\n",
    "    psds, fx = extract_power_series(cropped_data)\n",
    "    raw_power_series.append(psds)\n",
    "    #power_series.append(np.mean(pxx,axis=0))\n",
    "    #epoch_nums.append(pxx.shape([0]))\n",
    "    fxs.append(fx)\n",
    "    flat = psds.flatten()\n",
    "    flat_power_series.append(flat)\n",
    "    \n",
    "\n",
    "#power_series = np.stack(power_series)\n",
    "#fxs = np.stack(fxs)\n",
    "raw_power_series = np.array(raw_power_series)\n",
    "flat_power_series = np.array(flat_power_series)\n",
    "fxs = np.array(fxs)\n",
    "labels = np.array(labels)\n",
    "print(power_series.shape)\n",
    "print(fxs.shape)\n",
    "print(labels.shape)\n",
    "#epoch_nums = np.array(epoch_nums)\n",
    "#print(epoch_nums.shape)\n",
    "np.savez_compressed('extracted_features.npz',raw_psd=raw_power_series, flat_psd=flat_power_series, freqs=fxs,label=labels)\n",
    "#np.savez_compressed('epoch_counts.npz',epoch_counts=epoch_nums,label=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Discussion of your results and how they address your experimental question(s).\n",
    "* Discussed limitations of your analyses.\n",
    "* You can also discuss future directions you'd like to pursue."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e814ace6b585120c67a064ede6e75d74e71e2521d0263ff658efa26a1fa66ce"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
